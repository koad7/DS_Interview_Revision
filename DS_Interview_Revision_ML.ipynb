{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfs5Gpjx0u2X8FK4JJ8sjw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koad7/DS_Interview_Revision/blob/main/DS_Interview_Revision_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "Nm1VcoO9xvbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.1. Say you are building a binary classifier for an unbalanced dataset (where one class is rarer  than the other, say 1% and 99% respectively). How do you handle this situation?\n"
      ],
      "metadata": {
        "id": "F7QSd7cWrF9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution 1**: Try to find more data.\n",
        "\n",
        "**Solution 2**: Use appropriate performance metrics, Accuracy doesnt work well with imbalanced data. (Why?)\n",
        "\n",
        "**Solution 3**: Resambpl e the training set by either oversampling the rare samples or undersampling the abundant samples. You can use bootstrapping.\n",
        "\n",
        "**Solution 4**: Generate synthetic examples. e.g SMOTE\n",
        "\n",
        "**Solution 5**: Runing ensemble models with differnt rations of the classes or by running an ensemble model using alls amples of the rare class and a differeing amount of the abundant class.\n",
        "\n",
        "**Solution 6**: Design your own cost function that penalizes wrong classification of the rare class more than wrong classifications of the abundant class."
      ],
      "metadata": {
        "id": "3SG-9irhr-JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a dataset (replace this with your actual dataset)\n",
        "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.99, 0.01], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "# Splitting dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "MMSPKwrIzdS-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusting class weight"
      ],
      "metadata": {
        "id": "_6xu5Jr8zvih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize logistic regression with class_weight='balanced'\n",
        "clf = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Proceed with prediction and evaluation...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "XftLZdRrzu6j",
        "outputId": "07bf38ee-eae5-45b4-ef9d-9151779b3e72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(class_weight='balanced')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using appropriate evvaluation metrics"
      ],
      "metadata": {
        "id": "n8G1Ivu2z01d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdybMgFTz1Ss",
        "outputId": "bf9e59e7-8dc2-475c-dfca-0ad1ffa982ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       247\n",
            "           1       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00       250\n",
            "   macro avg       1.00      1.00      1.00       250\n",
            "weighted avg       1.00      1.00      1.00       250\n",
            "\n",
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble method like Random Forests or Gradient Boosting"
      ],
      "metadata": {
        "id": "sNRsF2280AX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize random forest with class_weight='balanced'\n",
        "rf = RandomForestClassifier(class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Proceed with prediction and evaluation...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "XP2kam8Mz_3S",
        "outputId": "bc97d322-300e-405e-d013-a539917b85af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(class_weight='balanced')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom probabvility threshold"
      ],
      "metadata": {
        "id": "-r4Fy7b80TV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = clf.predict_proba(X_test)[:, 1] # Probabilities for the positive class\n",
        "\n",
        "# Custom threshold\n",
        "threshold = 0.3\n",
        "y_pred_custom = np.where(y_probs > threshold, 1, 0)\n",
        "\n",
        "# Evaluation with custom threshold\n",
        "print(classification_report(y_test, y_pred_custom))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAx5SE6S0SXE",
        "outputId": "f2ac93ee-a34d-496e-81dd-5794b4b1f200"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       247\n",
            "           1       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           1.00       250\n",
            "   macro avg       0.88      1.00      0.93       250\n",
            "weighted avg       1.00      1.00      1.00       250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.2. What are some differences you would expect in a model that minimizes squared error versus a model that minimizes absolute error? In which cases would each error metric be appropriate?\n"
      ],
      "metadata": {
        "id": "erqEWQI2rM1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimizing squared error and minimizing absolute error are two different approaches to error measurement in predictive models, each with its unique characteristics and implications. Here are the key differences you would expect in models optimized for these two types of errors, along with scenarios where each is appropriate:\n",
        "\n",
        "###### Squared Error (L2 Loss)\n",
        "\n",
        "**Minimizing Squared Error** focuses on minimizing the square of the differences between predicted and actual values. This is commonly known as L2 loss or mean squared error (MSE).\n",
        "\n",
        "##### Characteristics:\n",
        "1. **More Sensitive to Outliers**: Squared error penalizes larger errors more heavily than smaller ones, as the error increases quadratically with the distance from the true value.\n",
        "2. **Differentiable**: This makes it easier to use in optimization algorithms that rely on gradient descent.\n",
        "3. **Tends to Have a Single Solution**: This can lead to more stable and predictable models.\n",
        "\n",
        "###### Appropriate Cases:\n",
        "- When the model needs to be particularly sensitive to larger errors.\n",
        "- In regression problems where a smooth loss landscape is beneficial (e.g., linear regression).\n",
        "- When the data distribution is Gaussian, as MSE corresponds to maximum likelihood estimation under this assumption.\n",
        "\n",
        "###### Example - Linear Regression Minimizing MSE:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming X_train, y_train are your training data\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "```\n",
        "\n",
        "**Absolute Error (L1 Loss)**\n",
        "\n",
        "**Minimizing Absolute Error** focuses on minimizing the absolute differences between predicted and actual values. This is known as L1 loss or mean absolute error (MAE).\n",
        "\n",
        "###### Characteristics:\n",
        "1. **Less Sensitive to Outliers**: L1 loss increases linearly with errors, making it more robust to outliers compared to L2 loss.\n",
        "2. **Can Have Multiple Optimal Solutions**: This can lead to a model that is less stable but potentially more robust.\n",
        "3. **Non-differentiable at Zero**: This can make optimization more challenging, particularly for methods that rely on gradient information.\n",
        "\n",
        "###### Appropriate Cases:\n",
        "- In scenarios where outliers are to be treated less harshly.\n",
        "- For distributions that are not well approximated by a Gaussian.\n",
        "- In problems where a median is more representative than a mean.\n",
        "\n",
        "###### Example - Linear Regression Minimizing MAE:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Custom loss function\n",
        "def mae_loss(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "# Assuming X_train, y_train are your training data\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mae_loss(y_test, predictions)\n",
        "```\n",
        "\n",
        "##### Choosing Between MSE and MAE\n",
        "\n",
        "The choice between MSE and MAE depends on the specific requirements of your problem and the nature of your data. If your data contains a lot of outliers and you want a measure that's\n",
        "\n",
        "robust to these outliers, MAE might be more appropriate. On the other hand, if you're dealing with data where the Gaussian assumption holds and you want to penalize larger errors more, MSE would be the better choice.\n",
        "\n",
        "It's also worth noting that while MSE can give more weight to larger errors (which could be either good or bad, depending on the context), MAE provides a more uniform treatment of all errors, making it a more 'democratic' measure in some senses.\n",
        "\n",
        "In practice, it's often beneficial to experiment with both types of loss functions and evaluate which one aligns better with the objectives of your specific problem. Additionally, consider the distribution of your data and the impact of outliers on your predictive modeling."
      ],
      "metadata": {
        "id": "6Up-SmPo5nI4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDdNpIJ10iym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Squared error is *MSE* and absolute error is *MAE*.\n",
        "\n",
        "\n",
        "The main difference is that errors are squared before being averaged in *MSE*, meaning there is a relatively high weight given to large errors.\n",
        "\n",
        "*MSE* is useful when large errors in the model are trying to be avoided. Outliers disproportionately affect MSE more than *MAE*. *MAE* is more robust to outliers.\n",
        "\n",
        "*MSE* is easier to use since the gradient calculation is more straightforward than that of *MAE*, which requires some linear programming to compute the gradient.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IaM7RMUOxhFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.3. Facebook: When performing K-means clustering, how do you choose k?\n"
      ],
      "metadata": {
        "id": "I323LnwLrPQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the right number of clusters (\\( k \\)) in K-means clustering is a critical step as it directly influences the results of the clustering process. There's no hard and fast rule for this, but several methods are commonly used to estimate a good value for \\( k \\):\n",
        "\n",
        "##### 1. The Elbow Method\n",
        "\n",
        "The Elbow Method involves plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use.\n",
        "\n",
        "###### Code Example using the Elbow Method:\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "X = # your data here\n",
        "\n",
        "# Calculate distortions for a range of number of clusters\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
        "    km.fit(X)\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "##### 2. The Silhouette Method\n",
        "\n",
        "The Silhouette Method measures how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "\n",
        "###### Code Example using the Silhouette Method:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Sample data\n",
        "X = # your data here\n",
        "\n",
        "# Range of clusters to try\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "    \n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "```\n",
        "\n",
        "##### 3. The Gap Statistic\n",
        "\n",
        "The Gap Statistic compares the total within intra-cluster variation for different values of \\( k \\) with their expected values under null reference distribution of the data.\n",
        "\n",
        "###### Code Example using the Gap Statistic:\n",
        "\n",
        "For the Gap Statistic, it's recommended to use an existing library like `gap_statistic` from `gap_stat` due to the complexity of the calculations.\n",
        "\n",
        "```python\n",
        "from gap_statistic import OptimalK\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = # your data here\n",
        "\n",
        "optimalK = OptimalK(parallel_backend='joblib')\n",
        "n_clusters = optimalK(X, cluster_array=np.arange(1, 11))\n",
        "\n",
        "print('Optimal clusters: ', n_clusters)\n",
        "```\n",
        "\n",
        "##### Choosing \\( k \\)\n",
        "\n",
        "In practice, you might need to combine these methods and use domain knowledge to determine the most appropriate number of clusters. Each method has its strengths and weaknesses, and the choice can depend on the specific characteristics of your data and the purpose of the clustering."
      ],
      "metadata": {
        "id": "GExnnLJD7kCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "\n",
        "# Generating sample data using make_blobs\n",
        "# This will create a dataset with a specified number of clusters (for illustration purposes)\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Range of clusters to try\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "# Calculate silhouette scores for different number of clusters\n",
        "silhouette_scores = {}\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    silhouette_scores[n_clusters] = silhouette_avg\n",
        "```\n",
        "\n",
        "silhouette_scores\n",
        "\n",
        "Here are the calculated average silhouette scores for each number of clusters, using the generated sample data:\n",
        "\n",
        "- For 2 clusters, the average silhouette score is: $ \\approx $ 0.7049787496083262\n",
        "- For 3 clusters, the average silhouette score is: $ \\approx $ 0.5882004012129721\n",
        "- For 4 clusters, the average silhouette score is: $ \\approx $ 0.6505186632729437\n",
        "- For 5 clusters, the average silhouette score is: $ \\approx $ 0.56376469026194\n",
        "- For 6 clusters, the average silhouette score is: $ \\approx $ 0.4504666294372765\n",
        "\n",
        "Based on these scores, 2 clusters provide the highest silhouette score, indicating that it might be the most appropriate choice for this dataset. Remember that the silhouette score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. In real-world scenarios, you should also consider domain knowledge and the specific context of the data when choosing the number of clusters."
      ],
      "metadata": {
        "id": "-D1pky9GPJ6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.4. How can you make your models more robust to outliers?\n"
      ],
      "metadata": {
        "id": "cQRZxmBdrRjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making your models more robust to outliers is crucial, especially in datasets where anomalies are common or when these outliers can significantly skew your results. Here are several strategies to improve the robustness of your models:\n",
        "\n",
        "##### 1. Data Preprocessing\n",
        "\n",
        "- **Outlier Detection and Removal**: Identify outliers using statistical methods (like Z-scores, IQR) or visualization techniques (like box plots) and remove them from your dataset.\n",
        "- **Transformation**: Apply transformations like log, square root, or Box-Cox transformations to reduce the impact of outliers.\n",
        "\n",
        "##### 2. Robust Scaling\n",
        "\n",
        "- **Robust Scaler**: Use scalers that are robust to outliers, like `RobustScaler` in Scikit-learn, which scales features using statistics that are robust to outliers.\n",
        "- **Normalization**: Normalize data so that the scale of outliers is reduced (e.g., Min-Max scaling).\n",
        "\n",
        "##### 3. Choosing Robust Models\n",
        "\n",
        "- **Robust Algorithms**: Use algorithms that are inherently robust to outliers, like Random Forests, or robust variants of algorithms (e.g., RANSAC regressor).\n",
        "- **Regularization**: Use regularization techniques (like L1 or L2 regularization) which can reduce the impact of outliers by penalizing large weights in linear models.\n",
        "\n",
        "##### 4. Ensemble Methods\n",
        "\n",
        "- **Bagging**: Use ensemble methods like Bagging which can reduce the variance and improve model robustness.\n",
        "- **Random Forests**: A type of ensemble method that is inherently robust to outliers.\n",
        "\n",
        "##### 5. Using Appropriate Loss Functions\n",
        "\n",
        "- **Robust Loss Functions**: Use loss functions that are less sensitive to outliers, such as Mean Absolute Error (MAE) instead of Mean Squared Error (MSE).\n",
        "\n",
        "##### 6. Data Resampling\n",
        "\n",
        "- **Stratified Sampling**: Ensure that outliers are proportionally represented in both training and testing datasets.\n",
        "\n",
        "##### 7. Cross-Validation\n",
        "\n",
        "- **Robust Cross-Validation**: Use cross-validation techniques that are robust to outliers, like stratified k-fold.\n",
        "\n",
        "##### Example Code for Some Strategies:\n",
        "\n",
        "###### Robust Scaler:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your data\n",
        "```\n",
        "\n",
        "###### Random Forest:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)  # X_train and y_train are your training data\n",
        "```\n",
        "\n",
        "###### Regularization (Lasso for L1):\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = Lasso(alpha=0.1)  # alpha is a regularization parameter\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Remember, the choice of strategy heavily depends on the nature of your data and the specific requirements of your task. In many cases, a combination of these strategies will yield the best results."
      ],
      "metadata": {
        "id": "cH-xsA5DL9n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.5. Say that you are running a multiple linear regression and that you have reason to believe that several of the predictors are correlated. How will the results of the regression be affected if several are indeed correlated? How would you deal with this problem?\n"
      ],
      "metadata": {
        "id": "x9p7BwckrTzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When several predictors in a multiple linear regression are correlated, this situation is known as multicollinearity. Here's how it affects the regression and ways to deal with it:\n",
        "\n",
        "##### Effects of Multicollinearity\n",
        "\n",
        "1. **Unreliable Coefficients**: Multicollinearity can lead to large variances for the coefficient estimates, making them unstable and unreliable.\n",
        "2. **Misleading Significance of Predictors**: It becomes difficult to determine the individual effect of correlated predictors, as their coefficients can be significantly altered.\n",
        "3. **Overfitting**: The model might fit the training data well but perform poorly on unseen data.\n",
        "4. **Difficulty in Interpreting**: Interpreting the model becomes challenging because the effect of one predictor is difficult to isolate from the effect of the others.\n",
        "\n",
        "##### Dealing with Multicollinearity\n",
        "\n",
        "1. **Feature Selection**:\n",
        "   - **Manual Selection**: Remove predictors that are known to be correlated.\n",
        "   - **Variance Inflation Factor (VIF)**: Calculate the VIF for each predictor; a VIF value greater than 5 or 10 indicates high multicollinearity.\n",
        "\n",
        "    ###### VIF Calculation Example:\n",
        "\n",
        "    ```python\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    from statsmodels.tools.tools import add_constant\n",
        "    import pandas as pd\n",
        "\n",
        "    # Assuming df is your DataFrame and it includes all the predictors\n",
        "    X = add_constant(df)\n",
        "\n",
        "    # Calculating VIF for each feature\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    print(vif_data)\n",
        "    ```\n",
        "\n",
        "2. **Principal Component Analysis (PCA)**:\n",
        "   - Use PCA to reduce the dimensionality of the data. PCA combines the predictors into a smaller set of uncorrelated components.\n",
        "   \n",
        "    ###### PCA Example:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Assuming X is your predictors\n",
        "    pca = PCA(n_components=3)  # n_components based on the desired number of features\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Then use X_pca for your regression\n",
        "    ```\n",
        "\n",
        "3. **Ridge or Lasso Regression**:\n",
        "   - Use regularization methods like Ridge or Lasso regression which can handle multicollinearity by shrinking the coefficients.\n",
        "   \n",
        "    ###### Ridge Regression Example:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.linear_model import Ridge\n",
        "\n",
        "    # Assuming X_train and y_train are your training data\n",
        "    ridge = Ridge(alpha=1.0)  # Alpha is a tuning parameter\n",
        "    ridge.fit(X_train, y_train)\n",
        "    ```\n",
        "\n",
        "4. **Partial Least Squares Regression (PLSR)**:\n",
        "   - PLSR is similar to PCA but also considers the response variable, making it suitable for cases with multicollinearity.\n",
        "   \n",
        "    ###### PLSR Example:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "    pls = PLSRegression(n_components=3)\n",
        "    pls.fit(X_train, y_train)\n",
        "    ```\n",
        "\n",
        "It's important to remember that each of these methods has its advantages and drawbacks. The choice of method depends on the specifics of your dataset and the research question at hand. Sometimes, a combination of these techniques might be necessary to effectively address the issue of multicollinearity."
      ],
      "metadata": {
        "id": "Ss-xDMR0T2UR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.6. Describe the motivation behind random forests. What are two ways in which they improve upon individual decision trees?\n"
      ],
      "metadata": {
        "id": "oaHA8ll3rVq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Motivation Behind Random Forests\n",
        "\n",
        "Random Forests are motivated by the desire to improve upon the performance and robustness of individual decision trees. Decision trees are simple and interpretable, but they have significant limitations, like a tendency to overfit to the training data. Random Forests address these limitations by combining the predictions of multiple decision trees, leading to better generalization and robustness.\n",
        "\n",
        "##### Improvements Over Individual Decision Trees\n",
        "\n",
        "1. **Reduction of Overfitting**:\n",
        "   - Individual decision trees often overfit the data; they're sensitive to noise and variations in the training set. Random Forests mitigate this by averaging the results of many trees, each built on different subsets of the data. This averaging process reduces the model's sensitivity to the noise and specific quirks of the training set, leading to better generalization to unseen data.\n",
        "\n",
        "2. **Increased Accuracy**:\n",
        "   - By combining the predictions of multiple trees (each potentially focusing on different aspects of the data), Random Forests often achieve higher accuracy than individual trees. The ensemble approach leverages the strengths of each tree, while their individual errors are likely to cancel out when averaged, leading to a more accurate overall prediction.\n",
        "\n",
        "3. **Handling Non-Linearity and Feature Interactions**:\n",
        "   - Random Forests, by virtue of aggregating multiple trees, can capture complex non-linear relationships and interactions between features more effectively than a single tree.\n",
        "\n",
        "##### Code Example\n",
        "\n",
        "Below is a basic example of using Random Forests for classification with Scikit-Learn:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42) # n_estimators is the number of trees\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "```\n",
        "\n",
        "This example demonstrates a basic application of a Random Forest classifier using the Iris dataset. It highlights the simplicity of using Random Forests in a similar manner to individual decision trees, while reaping the benefits of improved performance and robustness."
      ],
      "metadata": {
        "id": "_6pTwmGkbAHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.7. Given a large dataset of payment transactions, say we want to predict the likelihood of a given transaction being fraudulent. However, there are many rows with missing values for various columns. How would you deal with this?"
      ],
      "metadata": {
        "id": "yY6S6p6MrXxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with missing values in a dataset, especially in a scenario like predicting fraudulent transactions, is crucial for the performance of the predictive model. Here are some common strategies to handle missing data, along with code examples:\n",
        "\n",
        "##### 1. Removing Data\n",
        "\n",
        "This involves simply removing rows or columns with missing values. This approach is straightforward but can lead to loss of valuable information, especially if a significant portion of data is missing.\n",
        "\n",
        "###### Code Example - Dropping Rows/Columns:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Drop rows with any missing value\n",
        "df_dropped_rows = df.dropna()\n",
        "\n",
        "# Drop columns with any missing value\n",
        "df_dropped_columns = df.dropna(axis=1)\n",
        "```\n",
        "\n",
        "##### 2. Imputation\n",
        "\n",
        "Imputation involves filling in missing data with substitute values.\n",
        "\n",
        "- **Mean/Median/Mode Imputation**: This is suitable for numerical columns. Use the mean for normally distributed data, median for skewed data, and mode for categorical data.\n",
        "- **Model-Based Imputation**: Use other columns to predict missing values.\n",
        "\n",
        "###### Code Example - Simple Imputation:\n",
        "\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Mean imputation\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "df['column'] = mean_imputer.fit_transform(df[['column']])\n",
        "\n",
        "# Median imputation\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "df['column'] = median_imputer.fit_transform(df[['column']])\n",
        "\n",
        "# Mode imputation for categorical data\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df['column'] = mode_imputer.fit_transform(df[['column']])\n",
        "```\n",
        "\n",
        "##### 3. Using Algorithms that Support Missing Values\n",
        "\n",
        "Some algorithms can handle missing values inherently. For example, tree-based algorithms like Random Forests and Gradient Boosting can handle missing values without imputation.\n",
        "\n",
        "##### 4. Creating Missing Value Indicators\n",
        "\n",
        "Sometimes the fact that data is missing can be informative. You can create a new column that indicates whether data was missing.\n",
        "\n",
        "###### Code Example - Missing Value Indicator:\n",
        "\n",
        "```python\n",
        "df['column_missing'] = df['column'].isnull().astype(int)\n",
        "```\n",
        "\n",
        "##### 5. Advanced Imputation Techniques\n",
        "\n",
        "- **K-Nearest Neighbors (KNN) Imputation**: Fill missing values based on K-nearest neighbors.\n",
        "- **Multiple Imputation by Chained Equations (MICE)**: A sophisticated technique that models each feature with missing values as a function of other features in a round-robin fashion.\n",
        "\n",
        "###### Code Example - KNN Imputation:\n",
        "\n",
        "```python\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
        "```\n",
        "\n",
        "##### Choosing the\n",
        "\n",
        "Right Strategy:\n",
        "\n",
        "The choice of strategy largely depends on:\n",
        "- The nature of your data (categorical vs. numerical, distribution of values)\n",
        "- The amount of missing data\n",
        "- The importance of preserving all rows or columns vs. the importance of maintaining data integrity\n",
        "\n",
        "In the context of predicting fraudulent transactions, it's crucial to balance the need for complete data with the need to avoid introducing biases or inaccuracies. For instance, if missing values in certain columns are related to the nature of the transaction itself (e.g., some types of transactions might not include certain information), then imputing these values might introduce noise. On the other hand, if the missing values are randomly distributed, imputation could be a viable approach.\n",
        "\n",
        "In any case, it's advisable to perform exploratory data analysis to understand the patterns of missingness and to experiment with different strategies, evaluating their impact on the performance of your predictive models."
      ],
      "metadata": {
        "id": "8LFycpV6bo5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"cyan\">7.8 Say you are running a simple logistic regression to solve a problem but find the results to be unsatisfactory. What are some ways you might improve your model, or what other models might you look into using instead?</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "7PVQq6hUrpNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you find that a simple logistic regression model is yielding unsatisfactory results, there are several steps you can take to potentially improve your model, or you might consider exploring alternative models. Here are some strategies for improvement:\n",
        "\n",
        "##### Improving Logistic Regression\n",
        "\n",
        "1. **Feature Engineering**:\n",
        "   - Create new features based on your existing data.\n",
        "   - Perform transformations (like log, square, or interaction terms) to existing features.\n",
        "\n",
        "   ###### Code Example - Feature Transformation:\n",
        "\n",
        "   ```python\n",
        "   import numpy as np\n",
        "\n",
        "   # Assuming 'X' is your feature matrix and 'feature1' is a column in 'X'\n",
        "   X['log_feature1'] = np.log(X['feature1'] + 1)\n",
        "   X['feature1_squared'] = X['feature1'] ** 2\n",
        "   ```\n",
        "\n",
        "2. **Feature Selection**:\n",
        "   - Use techniques like Recursive Feature Elimination (RFE) to select features that contribute most to the prediction.\n",
        "   \n",
        "   ###### Code Example - RFE:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.feature_selection import RFE\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "   estimator = LogisticRegression()\n",
        "   selector = RFE(estimator, n_features_to_select=5, step=1)\n",
        "   selector = selector.fit(X, y)\n",
        "\n",
        "   X_selected = selector.transform(X)\n",
        "   ```\n",
        "\n",
        "3. **Hyperparameter Tuning**:\n",
        "   - Adjust hyperparameters like `C` (inverse of regularization strength) in logistic regression.\n",
        "\n",
        "   ###### Code Example - Hyperparameter Tuning:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "   param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "   grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "   grid_search.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "4. **Data Scaling**:\n",
        "   - Scale your features, especially if they're on different scales.\n",
        "\n",
        "   ###### Code Example - Data Scaling:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "   scaler = StandardScaler()\n",
        "   X_scaled = scaler.fit_transform(X)\n",
        "   ```\n",
        "\n",
        "##### Alternative Models\n",
        "\n",
        "If improving logistic regression doesn't yield better results, consider trying other models:\n",
        "\n",
        "1. **Random Forest**:\n",
        "   - Good for capturing non-linearities and interactions between features.\n",
        "   \n",
        "   ###### Code Example - Random Forest:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "   rf = RandomForestClassifier()\n",
        "   rf.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "2. **Support Vector Machines (SVM)**:\n",
        "   - Effective in high dimensional spaces.\n",
        "\n",
        "   ###### Code Example - SVM:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.svm import SVC\n",
        "\n",
        "   svm = SVC()\n",
        "   svm.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "3. **Gradient Boosting Machines (GBM)**:\n",
        "   - Powerful for handling a variety of data types.\n",
        "\n",
        "   ###### Code Example - Gradient Boosting:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "   gbm = GradientBoostingClassifier()\n",
        "   gbm.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "4. **Neural Networks**:\n",
        "   - Suitable for complex datasets with large amounts of data.\n",
        "\n",
        "   ###### Code Example - Neural Network:\n",
        "\n",
        "   ```python\n",
        "   from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "   nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
        "   nn.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "When switching models, it's important to understand the nature of your data and the problem you're trying to solve. Different models have different strengths and are suited to different types of data and prediction tasks. For example, if your data is highly non-linear or if there are complex interactions between features, tree-based models like Random Forests or Gradient Boosting Machines might perform better than logistic regression.\n",
        "\n",
        "Furthermore, it's crucial to validate your model thoroughly using techniques like cross-validation and to evaluate it using appropriate metrics for your problem domain. This process helps in understanding whether the changes you've made are genuinely improving the model's performance or if they're just fitting noise in the training data.\n",
        "\n",
        "Lastly, keep in mind that more complex models can be more difficult to interpret and might require more computational resources. Therefore, it's important to balance complexity with the practical constraints and requirements of your project."
      ],
      "metadata": {
        "id": "T2VbUaPtcHkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"cyan\">7.9. Say you were running a linear regression for a dataset but you accidentally duplicated every data point. What happens to your beta coefficient?</font>\n"
      ],
      "metadata": {
        "id": "ukGY4Or8rxxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicating every data point in a dataset for linear regression will not change the beta coefficients (slope parameters) of the model. The reason for this is that linear regression parameters are estimated based on the relationship between the independent and dependent variables, and this relationship doesn't change when you duplicate the data.\n",
        "\n",
        "However, duplicating the data points will affect other aspects of the regression output, such as the p-values and standard errors, making the model appear more significant than it actually is. This is because duplicating data points artificially inflates the sample size, misleadingly increasing the statistical power of the test.\n",
        "\n",
        "Let's demonstrate this with a code example. First, we'll fit a linear regression model to a dataset, and then we'll fit the same model to a duplicated version of the dataset and compare the beta coefficients.\n",
        "\n",
        "### Code Example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Generate some random data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 2 * X + 1 + np.random.randn(100, 1) * 0.5  # y = 2x + 1 + noise\n",
        "\n",
        "# Add a constant to the model (for the intercept term)\n",
        "X_sm = sm.add_constant(X)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = sm.OLS(y, X_sm).fit()\n",
        "original_coefs = model.params\n",
        "\n",
        "# Duplicate the data\n",
        "X_duplicated = np.concatenate([X, X])\n",
        "y_duplicated = np.concatenate([y, y])\n",
        "X_duplicated_sm = sm.add_constant(X_duplicated)\n",
        "\n",
        "# Fit the same model to the duplicated data\n",
        "model_duplicated = sm.OLS(y_duplicated, X_duplicated_sm).fit()\n",
        "duplicated_coefs = model_duplicated.params\n",
        "\n",
        "original_coefs, duplicated_coefs\n",
        "```\n",
        "\n",
        "In this example, `original_coefs` and `duplicated_coefs` should be very similar, indicating that the beta coefficients haven't changed significantly despite the duplication of data. However, if you were to look at the summary of both models (using `model.summary()` and `model_duplicated.summary()`), you would see differences in the standard errors, t\n",
        "\n",
        "As shown in the code example, the beta coefficients (`original_coefs` and `duplicated_coefs`) remain the same before and after duplicating the data points. Both models yield the coefficients:\n",
        "\n",
        "- Intercept: approximately 1.111\n",
        "- Slope: approximately 1.968\n",
        "\n",
        "This demonstrates that duplicating every data point in a dataset for linear regression does not affect the estimated coefficients of the model. However, as previously mentioned, this will impact other statistical measures like p-values and standard errors, which can lead to misleading interpretations of the model's significance and confidence."
      ],
      "metadata": {
        "id": "qYmwpi2getdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Generate some random data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 2 * X + 1 + np.random.randn(100, 1) * 0.5  # y = 2x + 1 + noise\n",
        "\n",
        "# Add a constant to the model (for the intercept term)\n",
        "X_sm = sm.add_constant(X)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = sm.OLS(y, X_sm).fit()\n",
        "original_coefs = model.params\n",
        "\n",
        "# Duplicate the data\n",
        "X_duplicated = np.concatenate([X, X])\n",
        "y_duplicated = np.concatenate([y, y])\n",
        "X_duplicated_sm = sm.add_constant(X_duplicated)\n",
        "\n",
        "# Fit the same model to the duplicated data\n",
        "model_duplicated = sm.OLS(y_duplicated, X_duplicated_sm).fit()\n",
        "duplicated_coefs = model_duplicated.params\n",
        "\n",
        "original_coefs, duplicated_coefs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PizmH4PtfhQb",
        "outputId": "52bbe738-fbf5-4b56-9a81-793643035c23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.11107554, 1.96846751]), array([1.11107554, 1.96846751]))"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"cyan\">7.10. Compare and contrast gradient boosting and random Forests.</font>\n"
      ],
      "metadata": {
        "id": "ue8-0949Pq0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting and Random Forests are both popular ensemble learning techniques used for regression and classification problems, but they operate in fundamentally different ways.\n",
        "\n",
        "##### Random Forests\n",
        "\n",
        "**Random Forests** is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "###### Key Characteristics:\n",
        "1. **Bagging Model**: Random Forests use the bagging technique, where each tree is built on a bootstrap sample (with replacement) from the training data.\n",
        "2. **Feature Subset**: In each split of the tree, only a random subset of features is considered.\n",
        "3. **Parallel Training**: Trees are built independently, making the process easily parallelizable.\n",
        "4. **Reduction of Variance**: Helps in reducing overfitting by averaging multiple trees.\n",
        "\n",
        "###### Example - Random Forest in Python:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_predictions = rf.predict(X_test)\n",
        "```\n",
        "\n",
        "##### Gradient Boosting\n",
        "\n",
        "**Gradient Boosting** is a boosting technique that builds trees one at a time, where each new tree helps to correct errors made by previously trained trees.\n",
        "\n",
        "###### Key Characteristics:\n",
        "1. **Boosting Model**: Gradient Boosting builds trees sequentially, where each tree tries to correct the errors of the previous one.\n",
        "2. **Gradient Descent**: It uses gradient descent to minimize the loss when adding new models.\n",
        "3. **Shrinkage**: Introduces a learning rate parameter that scales the contribution of each tree. Smaller values (shrinkage) generally lead to better models.\n",
        "4. **Reduction of Bias and Variance**: Focuses on reducing bias, though with enough trees, it can also achieve variance reduction.\n",
        "\n",
        "###### Example - Gradient Boosting in Python:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "gb_predictions = gb.predict(X_test)\n",
        "```\n",
        "\n",
        "##### Comparison\n",
        "\n",
        "- **Model Complexity**: Random Forests build complex models by combining the results of many independently built decision trees. In contrast, Gradient Boosting builds increasingly complex models by sequentially adding trees that correct the errors of the combined ensemble of previous trees.\n",
        "- **Training Speed**: Random Forests can be faster to train compared to Gradient Boosting because of parallel tree construction. Gradient Boosting often requires more careful tuning and longer training times.\n",
        "- **\n",
        "\n",
        "Overfitting Risk**: Random Forests are less likely to overfit if there are enough trees, due to their averaging nature. Gradient Boosting can overfit if the number of trees is too large or the learning rate is too high.\n",
        "- **Interpretability**: Both models are less interpretable than simple decision trees, but Random Forests generally offer a bit more interpretability since each tree can be considered independently.\n",
        "- **Performance Tuning**: Gradient Boosting often requires careful tuning of parameters like the number of trees, learning rate, and depth of trees. Random Forests are more robust to the choice of parameters, but tuning can still improve performance.\n",
        "- **Handling Noisy Data**: Random Forests can be more robust to noise, given their averaging nature. Gradient Boosting can be sensitive to outliers and noise.\n",
        "\n",
        "In practice, the choice between Random Forests and Gradient Boosting depends on the specific requirements of the application, the nature of the data, and the computational resources available. Gradient Boosting is often a go-to when performance is the primary concern and you have the time and resources to properly tune the model. Random Forests are a strong choice when you need a robust model quickly, with less tuning required."
      ],
      "metadata": {
        "id": "B-UMYnd1RX9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"cyan\">7.11. Say that DoorDash is launching in Singapore. For this new market, you want to predict the estimated time of arrival (BA) for a delivery to reach a customet afer an order has been placed on the app. From an earlier beta test in Singapore, there were 10,000 deliveries made. Do you have enough training data to create an accurate ETA model?</font>"
      ],
      "metadata": {
        "id": "bENpbqGYPxo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whether 10,000 deliveries provide enough data for an accurate Estimated Time of Arrival (ETA) model depends on various factors, including the complexity of the model, the diversity of the data, and the specific features captured in the dataset. Generally, 10,000 data points could be considered a reasonable starting point for building a predictive model, especially if the data is rich in informative features.\n",
        "\n",
        "For a more concrete assessment, we can simulate a scenario with a dataset of similar size and complexity to estimate the performance of a predictive model. Let's assume we're using a relatively simple model, like a Random Forest, for this purpose. We'll create a synthetic dataset with features that might be relevant for an ETA prediction model, such as distance to destination, time of day, day of the week, and weather conditions.\n",
        "\n",
        "### Code Example: Simulating a Model Training with a Dataset of 10,000 Deliveries\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 10000\n",
        "data = {\n",
        "    'distance': np.random.uniform(1, 10, n_samples),  # distance in kilometers\n",
        "    'time_of_day': np.random.randint(0, 24, n_samples),  # hour of the day\n",
        "    'day_of_week': np.random.randint(0, 7, n_samples),  # day of the week\n",
        "    'weather_condition': np.random.randint(0, 4, n_samples),  # 0: clear, 1: rainy, 2: foggy, 3: stormy\n",
        "    'ETA': np.random.uniform(10, 60, n_samples)  # ETA in minutes\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare the data\n",
        "X = df.drop('ETA', axis=1)\n",
        "y = df['ETA']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "```\n",
        "\n",
        "This example provides a basic framework to start with. In a real-world scenario, the model's accuracy would depend heavily on the quality and relevance of the features. For instance, in an ETA prediction model, traffic conditions, driver experience, and specific location data might be critical factors.\n",
        "\n",
        "If the model's performance is not satisfactory, additional data, more complex models, or feature engineering might be necessary. However, with 10,000 data points, you have a solid foundation to begin the modeling process and iteratively improve the model."
      ],
      "metadata": {
        "id": "Ql587gfZkHB-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOWNu38tPxdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Medium\n",
        "how would you supply these reasons?\n",
        "7.13. Google: Say you are given a very large corpus of words. How would you identify synonyms?\n",
        "7.14. Facebook: What is the bias-variance trade-off? How is it expressed using an equation?\n",
        "7.15. Uber: Define the cross-validation process. What is the motivation behind using it?\n",
        "7.16. Salesforce: How would you build a lead scoring algorithm to predict whether a prospective company is likely to convert into being an enterprise customer?\n",
        "7.17. Spotify: How would you approach creating a music recommendation algorithm?\n",
        "7.18. Amazon: Define what it means for a function to be convex. What is an example of a machine learning algorithm that is not convex and describe why that is so?\n",
        "7.19. Microsoft: Explain what information gain and entropy are in the context of a decision tree and walk through a numerical example.\n",
        "7.20. Uber: What is L1 and L2 regularization? What are the differences between the two?\n",
        "7.21. Amazon: Describe gradient descent and the motivations behind stochastic gradient descent.\n",
        "7.22. Affirm: Assume we have a classifier that produces a score between 0 and 1 for the probability of a particular loan application being fraudulent. Say that for each application's score, we take the square root of that score. How would the ROC curve change? If it doesn't change, what kinds of functions would change the curve?\n",
        "7.23. IBM: Say X is a univariate Gaussian random variable. What is the entropy of X?\n",
        "7.24. Stitch Fix: How would you build a model to calculate a customer's propensity to buy a particular item? What are some pros and cons of your approach?\n",
        "7.25. Citadel: Compare and contrast Gaussian Naive Bayes (GNB) and logistic regression. When would you use one over the other?\n",
        "lard\n",
        "26. Walmart: What loss function is used in k-means clustering given k clusters and n sample points? Compute the update formula using (1) batch gradient descent and (2) stochastic gradient descent for the cluster mean for cluster k using a learning rate c."
      ],
      "metadata": {
        "id": "uYqrcG39PqH8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGYIACWgzTyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When performing K-means clustering, how do you choose K?"
      ],
      "metadata": {
        "id": "OXlDroA2zURf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can you make your models more robust to outliers?"
      ],
      "metadata": {
        "id": "hrN5ehDjza-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say that you are running a multiple linear regression and that you have reason to believe that several of the predictors are correlated. Ho w will the results of the regression de affected if several are indeed correlated? How would you deal with this problem?"
      ],
      "metadata": {
        "id": "hc6N-qQ1zfmd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NW22NvBdzaJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the motivation behind random forested. What are two ways in which they improve upon individual decision trees?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Given a large dateset of payementn trnasactions, say we want to predict the likelihood of a given transasction being fraudulent. however there are many rows with missing values for various columns Hwo would you deal with this?"
      ],
      "metadata": {
        "id": "y7PhEkCozyM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT\n",
        "  COUNT DISTINCT app_id\n",
        "WHERE event_ID IS \"clicktrough rate\"\n",
        "AND YEAR(timestamp)  iS \"2019\";"
      ],
      "metadata": {
        "id": "NogSRRv-0K-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UZW7_cdG0Lh_"
      }
    }
  ]
}